{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap as lsc\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "from deeplsd.utils.tensor import batch_to_device\n",
    "from deeplsd.models.deeplsd_inference import DeepLSD\n",
    "from deeplsd.geometry.viz_2d import plot_images, plot_lines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "640\n"
     ]
    }
   ],
   "source": [
    "# Load an image\n",
    "img = cv2.imread('../assets/images/example.jpg')[:, :, ::-1]\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Model config\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "conf = {\n",
    "    'detect_lines': True,  # Whether to detect lines or only DF/AF\n",
    "    'line_detection_params': {\n",
    "        'merge': False,  # Whether to merge close-by lines\n",
    "        'filtering': True,  # Whether to filter out lines based on the DF/AF. Use 'strict' to get an even stricter filtering\n",
    "        'grad_thresh': 3,\n",
    "        'grad_nfa': True,  # If True, use the image gradient and the NFA score of LSD to further threshold lines. We recommand using it for easy images, but to turn it off for challenging images (e.g. night, foggy, blurry images)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load the model\n",
    "ckpt = '../weights/deeplsd_md.tar'\n",
    "ckpt = torch.load(str(ckpt), map_location='cpu')\n",
    "net = DeepLSD(conf)\n",
    "net.load_state_dict(ckpt['model'])\n",
    "net = net.to(device).eval()\n",
    "\n",
    "\n",
    "H, W = gray_img.shape  # 获取高度和宽度\n",
    "print(H)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "  %lines : Float(1, 0, strides=[1, 1], requires_grad=0, device=cpu) = onnx::Constant[value=[ CPUFloatType{1,0} ]]()\n",
      "  return (%lines)\n",
      "\n",
      "Model has been converted to ONNX\n"
     ]
    }
   ],
   "source": [
    "# 假设 net 是你的模型，gray_img 是你的输入图像\n",
    "dummy_input = torch.randn(1, 1, 480, 640) \n",
    "#inputs = {'image': torch.tensor(gray_img, dtype=torch.float, device=device)[None, None] / 255.}\n",
    "inputs= {'image':dummy_input}\n",
    "#inputs = torch.randn(1,1,224,224)\n",
    "\n",
    "# 假设 net 是你的模型，gray_img 是你的输入图像\n",
    "#input_tensor = torch.tensor(gray_img, dtype=torch.float, device=device)[None, None] / 255.\n",
    "\n",
    "# 将输入从字典转换为元组\n",
    "#inputs = (input_tensor,)\n",
    "\n",
    "# 将模型导出为 ONNX 格式\n",
    "torch.onnx.export(net, \n",
    "                  inputs, \n",
    "                  \"DeepLSD.onnx\",\n",
    "                  input_names= [\"image\"],\n",
    "                  output_names = [\"lines\"],\n",
    "                  verbose = True\n",
    "                 )\n",
    "\n",
    "print(\"Model has been converted to ONNX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 数组:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "PyTorch 张量:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 创建一个 NumPy 数组\n",
    "np_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 将 NumPy 数组转换为 PyTorch 张量\n",
    "torch_tensor = torch.tensor(np_array, dtype=torch.float)\n",
    "\n",
    "print(\"NumPy 数组:\")\n",
    "print(np_array)\n",
    "\n",
    "print(\"\\nPyTorch 张量:\")\n",
    "print(torch_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img():\n",
    "    img = Image.open(\"../assets/images/example.jpg\").convert('L').resize((480, 640))\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input names: []\n"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession('DeepLSD.onnx')\n",
    "# 检查模型的输入信息\n",
    "input_names = [input.name for input in session.get_inputs()]\n",
    "print(\"Model input names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run(): incompatible function arguments. The following argument types are supported:\n    1. (self: onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession, arg0: List[str], arg1: Dict[str, object], arg2: onnxruntime.capi.onnxruntime_pybind11_state.RunOptions) -> List[object]\n\nInvoked with: <onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession object at 0x7f78ec48c9f0>, ['145'], array([[[[0.39215687, 0.40392157, 0.40392157, ..., 0.4392157 ,\n          0.43529412, 0.44313726],\n         [0.39215687, 0.40392157, 0.40392157, ..., 0.4392157 ,\n          0.43529412, 0.44313726],\n         [0.4       , 0.40392157, 0.40392157, ..., 0.4392157 ,\n          0.43529412, 0.44313726],\n         ...,\n         [0.4       , 0.39607844, 0.3764706 , ..., 0.9411765 ,\n          0.95686275, 0.94509804],\n         [0.36862746, 0.3647059 , 0.34117648, ..., 0.94509804,\n          0.9529412 , 0.9529412 ],\n         [0.3019608 , 0.29411766, 0.28627452, ..., 0.9490196 ,\n          0.94509804, 0.9529412 ]]]], dtype=float32), None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8094/499062466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(outs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: run(): incompatible function arguments. The following argument types are supported:\n    1. (self: onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession, arg0: List[str], arg1: Dict[str, object], arg2: onnxruntime.capi.onnxruntime_pybind11_state.RunOptions) -> List[object]\n\nInvoked with: <onnxruntime.capi.onnxruntime_pybind11_state.InferenceSession object at 0x7f78ec48c9f0>, ['145'], array([[[[0.39215687, 0.40392157, 0.40392157, ..., 0.4392157 ,\n          0.43529412, 0.44313726],\n         [0.39215687, 0.40392157, 0.40392157, ..., 0.4392157 ,\n          0.43529412, 0.44313726],\n         [0.4       , 0.40392157, 0.40392157, ..., 0.4392157 ,\n          0.43529412, 0.44313726],\n         ...,\n         [0.4       , 0.39607844, 0.3764706 , ..., 0.9411765 ,\n          0.95686275, 0.94509804],\n         [0.36862746, 0.3647059 , 0.34117648, ..., 0.94509804,\n          0.9529412 , 0.9529412 ],\n         [0.3019608 , 0.29411766, 0.28627452, ..., 0.9490196 ,\n          0.94509804, 0.9529412 ]]]], dtype=float32), None"
     ]
    }
   ],
   "source": [
    "img = load_img()\n",
    "session = onnxruntime.InferenceSession('DeepLSD.onnx')\n",
    "inputs = img.numpy()\n",
    "#print(inputs)\n",
    "outs = session.run(None,inputs)\n",
    "#print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
